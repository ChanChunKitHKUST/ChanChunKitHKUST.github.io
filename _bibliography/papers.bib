---
---
@article{DBLP:journals/corr/abs-2304-14827,
  abbr={Findings-EACL},
  bibtex_show={true},
  selected={true},
  author       = {Chunkit Chan and
                  Jiayang Cheng and
                  Weiqi Wang and
                  Yuxin Jiang and
                  Tianqing Fang and
                  Xin Liu and
                  Yangqiu Song},
  title        = {ChatGPT Evaluation on Sentence Level Relations: {A} Focus on Temporal,
                  Causal, and Discourse Relations},
  journal      = {Findings-EACL2024},
  volume       = {abs/2304.14827},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2304.14827},
  doi          = {10.48550/arXiv.2304.14827},
  eprinttype    = {arXiv},
  eprint       = {2304.14827},
  timestamp    = {Thu, 04 May 2023 15:47:42 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2304-14827.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/acl/ChanLCLSWS23,
  abbr={Findings-ACL},
  bibtex_show={true},
  selected={true},
  author       = {Chunkit Chan and
                  Xin Liu and
                  Jiayang Cheng and
                  Zihan Li and
                  Yangqiu Song and
                  Ginny Y. Wong and
                  Simon See},
  editor       = {Anna Rogers and
                  Jordan L. Boyd{-}Graber and
                  Naoaki Okazaki},
  title        = {DiscoPrompt: Path Prediction Prompt Tuning for Implicit Discourse
                  Relation Recognition},
  booktitle    = {Findings of the Association for Computational Linguistics: {ACL} 2023,
                  Toronto, Canada, July 9-14, 2023},
  pages        = {35--57},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.findings-acl.4},
  doi          = {10.18653/v1/2023.findings-acl.4},
  timestamp    = {Thu, 10 Aug 2023 12:35:51 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/ChanLCLSWS23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{chan-etal-2023-self,
    abbr={Oral-AACL},
    selected={true},
    bibtex_show={true},
    title = "Self-Consistent Narrative Prompts on Abductive Natural Language Inference",
    author = "Chan, Chunkit  and
      Liu, Xin  and
      Chan, Tsz Ho  and
      Cheng, Jiayang  and
      Song, Yangqiu  and
      Wong, Ginny  and
      See, Simon",
    editor = "Park, Jong C.  and
      Arase, Yuki  and
      Hu, Baotian  and
      Lu, Wei  and
      Wijaya, Derry  and
      Purwarianti, Ayu  and
      Krisnadhi, Adila Alfa",
    booktitle = "Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = nov,
    year = "2023",
    address = "Nusa Dua, Bali",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.ijcnlp-main.67",
    pages = "1040--1057",
}

@inproceedings{jiang-etal-2023-lion,
    abbr={Oral-EMNLP},
    bibtex_show={true},
    selected={true},
    title = "Lion: Adversarial Distillation of Proprietary Large Language Models",
    author = "Jiang, Yuxin  and
      Chan, Chunkit  and
      Chen, Mingyang  and
      Wang, Wei",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.189",
    doi = "10.18653/v1/2023.emnlp-main.189",
    pages = "3134--3154",
    abstract = "The practice of transferring knowledge from a sophisticated, proprietary large language model (LLM) to a compact, open-source LLM has garnered considerable attention. Previous works have focused on a unidirectional knowledge distillation way by aligning the responses of the student model with those of the teacher model to a set of instructions. Nevertheless, they overlooked the possibility of incorporating any {``}feedback{''}{--}identifying challenging instructions where the student model{'}s performance falls short{--}to boost the student model{'}s proficiency iteratively. To this end, we propose a novel adversarial distillation framework for a more efficient knowledge transfer. Leveraging the versatile role adaptability of LLMs, we prompt the teacher model to identify {``}hard{''} instructions and generate new {``}hard{''} instructions for the student model, creating a three-stage adversarial loop of imitation, discrimination, and generation. By applying this adversarial framework, we successfully transfer knowledge from ChatGPT to a student model (named Lion), using a mere 70k training data. Our results show that Lion-13B not only achieves comparable open-ended generation capabilities to ChatGPT but surpasses conventional state-of-the-art (SOTA) instruction-tuned models like Vicuna-13B by 55.4{\%} in challenging zero-shot reasoning benchmarks such as BIG-Bench Hard (BBH) and 16.7{\%} on AGIEval.",
}

@inproceedings{jiayang-etal-2023-storyanalogy,
      abbr={Main-EMNLP},
      bibtex_show={true},
      selected={true},
    title = "{S}tory{A}nalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding",
    author = "Jiayang, Cheng  and
      Qiu, Lin  and
      Chan, Tsz  and
      Fang, Tianqing  and
      Wang, Weiqi  and
      Chan, Chunkit  and
      Ru, Dongyu  and
      Guo, Qipeng  and
      Zhang, Hongming  and
      Song, Yangqiu  and
      Zhang, Yue  and
      Zhang, Zheng",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.706",
    doi = "10.18653/v1/2023.emnlp-main.706",
    pages = "11518--11537",
    abstract = "Analogy-making between narratives is crucial for human reasoning. In this paper, we evaluate the ability to identify and generate analogies by constructing a first-of-its-kind large-scale story-level analogy corpus, StoryAnalogy, which contains 24K story pairs from diverse domains with human annotations on two similarities from the extended Structure-Mapping Theory. We design a set of tests on StoryAnalogy, presenting the first evaluation of story-level analogy identification and generation. Interestingly, we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around 30{\%} accuracy in multiple-choice questions (compared to over 85{\%} accuracy for humans). Furthermore, we observe that the data in StoryAnalogy can improve the quality of analogy generation in LLMs, where a fine-tuned FlanT5-xxl model achieves comparable performance to zero-shot ChatGPT.",
}

@article{li2023pbench,
      abbr={Arxiv},
      bibtex_show={true},
      title={P-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models}, 
      author={Haoran Li and Dadi Guo and Donghao Li and Wei Fan and Qi Hu and Xin Liu and Chunkit Chan and Duanyi Yao and Yangqiu Song},
      year={2023},
      eprint={2311.04044},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{li2023privacy,
      abbr={Arxiv},
      bibtex_show={true},
      title={Privacy in Large Language Models: Attacks, Defenses and Future Directions}, 
      author={Haoran Li and Yulin Chen and Jinglong Luo and Yan Kang and Xiaojin Zhang and Qi Hu and Chunkit Chan and Yangqiu Song},
      year={2023},
      eprint={2310.10383},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2024candle,
      abbr={Arxiv},
      bibtex_show={true},
      title={CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning}, 
      author={Weiqi Wang and Tianqing Fang and Chunyang Li and Haochen Shi and Wenxuan Ding and Baixuan Xu and Zhaowei Wang and Jiaxin Bai and Xin Liu and Jiayang Cheng and Chunkit Chan and Yangqiu Song},
      year={2024},
      eprint={2401.07286},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Cheng2024EventGroundNR,
      abbr={Main-COLING},
      bibtex_show={true},
      selected={true},
      journal = {COLING2024},
  title={EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs},
  author={Jiayang Cheng and Lin Qiu and Chunkit Chan and Xin Liu and Yangqiu Song and Zheng Zhang},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:268819426}
}


